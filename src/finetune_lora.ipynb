{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78732590-6905-4d4d-8191-3e3a8064f938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.55.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.12/site-packages (0.17.1)\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.12/site-packages (0.21.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.12/site-packages (from peft) (2.6.0+cu124)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from peft) (1.10.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.12/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.12/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (5.28.3)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.12/site-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (2.35.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /opt/conda/lib/python3.12/site-packages (from wandb) (4.14.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<3->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets peft wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715deb94-e2b7-4080-a288-2b08bfc78b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets \n",
    "import peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10056aa6-814d-4528-af4d-e9c18f90fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4, # number of the parameters to train.\n",
    "    lora_alpha=1, # magnitude of the weight matrix\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"lora_only\",\n",
    "    task_type=\"SEQ_CLS\" # sequence classification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "702863a2-90f2-44b2-b739-3ccd4edca611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "model_checkpoint = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "foundation_model = BertModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb3a8d1d-ca26-407c-bb84-6ad83dbc48c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"/home/jovyan/work/ULM-25-authorship-profiling/data/\"\n",
    "\n",
    "df_train = pd.read_csv(DATA_PATH + \"data_train.csv\")\n",
    "df_test = pd.read_csv(DATA_PATH + \"data_test.csv\")\n",
    "df_val = pd.read_csv(DATA_PATH + \"data_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a113dce-7650-4f68-b152-2ad6aed40758",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict()\n",
    "dataset['train'] = Dataset.from_pandas(df_train)\n",
    "dataset['validation'] = Dataset.from_pandas(df_val)\n",
    "dataset['test'] = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc42e57b-d9f5-4887-bd7e-83f7177a0e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0298536da140a3b5dda6615ab4ddcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/620813 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a65455f5bf6478e9d40093d3385d928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/68980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e428ee6e43743d79672b53f462269ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/37919 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'age', 'gender'],\n",
       "        num_rows: 620812\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'age', 'gender'],\n",
       "        num_rows: 68980\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'age', 'gender'],\n",
       "        num_rows: 37919\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda example: \n",
    "                         example[\"text\"] is not None and \n",
    "                         example[\"gender\"] is not None and\n",
    "                         example [\"age\"] is not None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3fa914d-b611-42d5-8e43-5a869a8219a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, PreTrainedModel, PretrainedConfig\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "age_encoder = LabelEncoder()\n",
    "age_encoder.fit(df_train[\"age\"].tolist() + df_val[\"age\"].tolist() + df_test[\"age\"].tolist())\n",
    "num_age_labels = len(age_encoder.classes_)\n",
    "\n",
    "BINS = [0, 18, 23, 27, 33, 37, 43, 47, 53, 57, 100]\n",
    "def preprocess_function_dict(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512\n",
    "    )\n",
    "    # age_labels = age_encoder.transform(examples[\"age\"])\n",
    "    age_labels = [\n",
    "            int(np.digitize(item, BINS) - 1)\n",
    "            for item in examples[\"age\"]\n",
    "        ]\n",
    "    gender_labels = [\n",
    "        {\"male\": 0, \"female\": 1}[label] for label in examples[\"gender\"]\n",
    "    ]\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"age_labels\": age_labels,\n",
    "        \"gender_labels\": gender_labels,\n",
    "        \"labels\": np.stack([age_labels, gender_labels], axis=1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed3d3872-208a-49f7-afd2-225f636bb525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c58719a4cd49bfb617eb50143c5c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/620812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2bca5f3e2cb4409b9e39b9da377e9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/68980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a8078b1d594dbebd1f41f4839d0530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37919 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function_dict,\n",
    "    batched=True,\n",
    "    remove_columns=['text', 'age', 'gender']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc8302aa-1620-46f6-bc49-ccfe20d77151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'age_labels', 'gender_labels', 'labels'],\n",
       "        num_rows: 620812\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'age_labels', 'gender_labels', 'labels'],\n",
       "        num_rows: 68980\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'age_labels', 'gender_labels', 'labels'],\n",
       "        num_rows: 37919\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce8604c1-deb4-4ed2-a9d5-1d0f14a9ddbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f510c4141f4d6b96e72f1e3220e375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58310c36-ce11-4632-9193-32184e1b0e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4d11ffe68046f9931376b3bb7ebc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8212b455257b4d16b4aa23872cce46a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/156 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09939f0a94f4b80a63244db17d0f8ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/156 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12b8e4d28804ac99266d689ee5cf6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/156 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e416074bb94f08b8e9ba8b1a8d6d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/156 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f9be3651ed4cfa8c8c7efcdfcea498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6302ccf57e45ceb32cae5bc7775fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/69 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee92284abab745cf89f0ca01c0dedd86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145addcb009f422687a9ac9c41f09fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/38 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/KonradBRG/ULM-Profiling-tokenized/commit/da3151e8ae81d36c4b16cab46907282cf7fbab93', commit_message='Upload tokenized ULM profiling dataset', commit_description='', oid='da3151e8ae81d36c4b16cab46907282cf7fbab93', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/KonradBRG/ULM-Profiling-tokenized', endpoint='https://huggingface.co', repo_type='dataset', repo_id='KonradBRG/ULM-Profiling-tokenized'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset.push_to_hub(\n",
    "    \"KonradBRG/ULM-Profiling-tokenized\",\n",
    "    private=False,  # or True if you want it private\n",
    "    commit_message=\"Upload tokenized ULM profiling dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d011ade-7090-40b2-a5af-a1ebda786a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointClassificationConfig(PretrainedConfig):\n",
    "    def __init__(self, num_age_labels=None, num_gender_labels=None, loss_alpha=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_age_labels = num_age_labels\n",
    "        self.num_gender_labels = num_gender_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c45c6b1c-4976-4168-8441-dfd04fe8c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class BertForJointClassification(PreTrainedModel):\n",
    "    config_class = JointClassificationConfig\n",
    "    \n",
    "    def __init__(self, config, model):\n",
    "        super().__init__(config)\n",
    "        self.num_age_labels = config.num_age_labels\n",
    "        self.num_gender_labels = config.num_gender_labels\n",
    "\n",
    "        self.bert = model\n",
    "        self.age_classifier = nn.Linear(config.hidden_size, self.num_age_labels)\n",
    "        self.gender_classifier = nn.Linear(config.hidden_size, self.num_gender_labels)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.age_loss_fct = CrossEntropyLoss()\n",
    "        self.gender_loss_fct = CrossEntropyLoss()\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        age_labels=None,\n",
    "        gender_labels=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # get BERT outputs\n",
    "        x = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        # pool outputs to get sequence representation\n",
    "        x = x.last_hidden_state[:, 0]\n",
    "        x = self.dropout(x)\n",
    "        # get logits from task head\n",
    "        age_logits = self.age_classifier(x)\n",
    "        gender_logits = self.gender_classifier(x)\n",
    "        loss_age = self.age_loss_fct(age_logits, age_labels)\n",
    "        loss_gender = self.gender_loss_fct(gender_logits, gender_labels)\n",
    "        return SequenceClassifierOutput(loss={\"loss_age\": loss_age, \"loss_gender\": loss_gender}, \n",
    "                                        logits=torch.cat([age_logits, gender_logits], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b31c1b07-691a-457b-88f2-59b2d4439839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(dataset, model_name=\"bert-base-uncased\", peft=True):        \n",
    "    config = JointClassificationConfig(\n",
    "        name_or_path=model_name,\n",
    "        num_age_labels=num_age_labels,\n",
    "        num_gender_labels=2,\n",
    "        hidden_size=768,\n",
    "        hidden_dropout_prob=0.1,\n",
    "    )\n",
    "    bert = BertForJointClassification(config, foundation_model)\n",
    "    if peft:\n",
    "        bert = get_peft_model(bert, lora_config)\n",
    "    return bert, config, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368fe62-6d89-4166-ae23-ccc138b9418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "peft_model.unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685989c-03f1-4fac-b453-3642d256707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model, config, data = setup_model(tokenized_dataset, model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bca6fa20-5e11-476f-ab54-6b65f9bbbd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    age_labels, gender_labels = eval_pred.label_ids\n",
    "\n",
    "    age_logits = logits[:, :num_age_labels]\n",
    "    gender_logits = logits[:, num_age_labels:]\n",
    "\n",
    "    age_preds = np.argmax(age_logits, axis=-1)\n",
    "    gender_preds = np.argmax(gender_logits, axis=-1)\n",
    "\n",
    "    age_acc = (age_preds == age_labels).mean()\n",
    "    gender_acc = (gender_preds == gender_labels).mean()\n",
    "    joint_acc = np.mean((age_preds == age_labels) & (gender_preds == gender_labels))\n",
    "\n",
    "    return {\"age_acc\": age_acc, \"gender_acc\": gender_acc, \"joint_acc\": joint_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a993361f-bc73-475b-8b4c-3fca37200ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Union, Any\n",
    "from transformers import Trainer, TrainingArguments, default_data_collator\n",
    "\n",
    "class TrainerWithCustomLoss(Trainer):\n",
    "    \n",
    "    def __init__(self, age_alpha: float = 0.5, scale_losses = True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.age_alpha = age_alpha\n",
    "        self.gender_alpha = 1 - self.age_alpha\n",
    "        self._scale_losses = scale_losses\n",
    "    \n",
    "    def training_step(\n",
    "        self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]], num_items_in_batch=None\n",
    "    ) -> torch.Tensor:\n",
    "        model.train()\n",
    "        if hasattr(self.optimizer, \"train\") and callable(self.optimizer.train):\n",
    "            self.optimizer.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        \n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "        self.accelerator.backward(loss)\n",
    "        \n",
    "        # Finally we need to normalize the loss for reporting\n",
    "        if num_items_in_batch is None:\n",
    "            return loss.detach() / self.args.gradient_accumulation_steps\n",
    "        return loss.detach()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs = model(**inputs)\n",
    "        loss_age, loss_gender = outputs.loss[\"loss_age\"], outputs.loss[\"loss_gender\"]\n",
    "        loss = self.age_alpha * loss_age + self.gender_alpha * loss_gender\n",
    "        \n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d4bc2-659b-4e5b-b954-0e1dcb6739aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fcce74-55d9-4739-8a7a-5f5d81b9e589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, default_data_collator\n",
    "\n",
    "print(peft_model.print_trainable_parameters())\n",
    "log_frequency = 1000\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"bert-lora-for-author-profiling\",\n",
    "    per_device_train_batch_size=32,  # Set explicitly for better control\n",
    "    gradient_accumulation_steps=2,   # Effective batch size = 32 * 2 = 64\n",
    "    num_train_epochs=3,\n",
    "    # Speed optimizations\n",
    "    dataloader_num_workers=4,        # Parallel data loading\n",
    "    dataloader_pin_memory=True,      # Faster GPU transfer\n",
    "    bf16=True,                       # Mixed precision (if you have newer GPU)\n",
    "    # fp16=True,                     # Use this instead if bf16 not supported\n",
    "    # Reduce overhead\n",
    "    eval_strategy=\"steps\",           # Less frequent evaluation\n",
    "    eval_steps=500,                  # Adjust based on dataset size\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=log_frequency,\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=5e-5,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "trainer = TrainerWithCustomLoss(\n",
    "    age_alpha=0.7,\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=data['train'].with_format(\"torch\"),\n",
    "    eval_dataset=data['validation'].with_format(\"torch\"),\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e83e7b2-6915-4b4b-8dfa-8bd23e360e1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d53dba2-cbf4-4b98-b9ff-cdc0260d112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(data[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd34786c-4926-4414-8b92-03a3b4ef91cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: egrnddjo\n",
      "Sweep URL: https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()\n",
    "sweep_config = {\n",
    "    \"name\": \"LoRA-For-Author-Profiling\",\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"Joint Acc\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"min\": 0.00005,\n",
    "            \"max\": 0.0001\n",
    "        },\n",
    "        \"age_alpha\": {\n",
    "            \"min\": 0.4,\n",
    "            \"max\": 0.9\n",
    "        },\n",
    "        \"per_device_train_batch_size\": {\n",
    "            \"values\": [16, 32, 64]\n",
    "        },\n",
    "        \"num_train_epochs\": {\n",
    "            \"values\": [2, 3, 4]\n",
    "        },\n",
    "        \"peft_r_value\": {\n",
    "            \"values\": [2, 4, 8]\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"ULM-Author-Profiling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0cda984-791e-4f17-a346-2bdf735211e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    # Initialize wandb run with the config from sweep\n",
    "    with wandb.init(config=config):\n",
    "        peft_r_value = wandb.config.peft_r_value\n",
    "        learning_rate = wandb.config.learning_rate\n",
    "        age_alpha = wandb.config.age_alpha\n",
    "        batch_size = wandb.config.per_device_train_batch_size\n",
    "        epochs = wandb.config.num_train_epochs\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r=peft_r_value, # number of the parameters to train.\n",
    "            lora_alpha=1, # magnitude of the weight matrix\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"lora_only\",\n",
    "            task_type=\"SEQ_CLS\" # sequence classification\n",
    "        )\n",
    "        peft_model, config, data = setup_model(tokenized_dataset, model_checkpoint)\n",
    "\n",
    "        # Set up training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"bert-lora-for-author-profiling\",\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            gradient_accumulation_steps=2,\n",
    "            num_train_epochs=epochs,\n",
    "            # Speed optimizations\n",
    "            dataloader_num_workers=4,    \n",
    "            dataloader_pin_memory=True,\n",
    "            bf16=True,\n",
    "            eval_strategy=\"steps\",         \n",
    "            eval_steps=1000,                  \n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=1000,\n",
    "            save_strategy=\"no\",\n",
    "            learning_rate=learning_rate,\n",
    "            report_to=\"wandb\"\n",
    "        )\n",
    "        \n",
    "        trainer = TrainerWithCustomLoss(\n",
    "            age_alpha=age_alpha,\n",
    "            model=peft_model,\n",
    "            args=training_args,\n",
    "            train_dataset=data['train'].with_format(\"torch\"),  # Make sure data is defined\n",
    "            eval_dataset=data['validation'].with_format(\"torch\"),\n",
    "            data_collator=default_data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        try:\n",
    "            trainer.train()\n",
    "            eval_results = trainer.evaluate()\n",
    "            wandb.log(eval_results)\n",
    "            print(f\"Training completed! Final Joint Acc: {eval_results.get('eval_Joint Acc', 'N/A')}\")\n",
    "        finally:\n",
    "            # Ensure cleanup happens even if training fails\n",
    "            peft_model.unload()\n",
    "            del trainer\n",
    "            del peft_model\n",
    "            torch.cuda.empty_cache()  # Clear GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d36ec29-ec06-490e-aa10-e69ab197b1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oyjz2f8m with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tage_alpha: 0.5256867795052806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8.377865352728644e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpeft_r_value: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkonrad-brg\u001b[0m (\u001b[33mkonrad-brg-university-of-t-bingen\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/work/ULM-25-authorship-profiling/src/wandb/run-20250828_194432-oyjz2f8m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/oyjz2f8m' target=\"_blank\">apricot-sweep-1</a></strong> to <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/oyjz2f8m' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/oyjz2f8m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29103' max='29103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29103/29103 2:27:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Age Acc</th>\n",
       "      <th>Gender Acc</th>\n",
       "      <th>Joint Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.059100</td>\n",
       "      <td>0.962221</td>\n",
       "      <td>0.532183</td>\n",
       "      <td>0.643592</td>\n",
       "      <td>0.342723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.951700</td>\n",
       "      <td>0.927538</td>\n",
       "      <td>0.553552</td>\n",
       "      <td>0.662163</td>\n",
       "      <td>0.368310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.929700</td>\n",
       "      <td>0.910950</td>\n",
       "      <td>0.561090</td>\n",
       "      <td>0.669730</td>\n",
       "      <td>0.378095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>0.902783</td>\n",
       "      <td>0.565700</td>\n",
       "      <td>0.674543</td>\n",
       "      <td>0.383734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.912200</td>\n",
       "      <td>0.896584</td>\n",
       "      <td>0.568484</td>\n",
       "      <td>0.677370</td>\n",
       "      <td>0.388301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.900400</td>\n",
       "      <td>0.890600</td>\n",
       "      <td>0.572369</td>\n",
       "      <td>0.681038</td>\n",
       "      <td>0.393563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.895900</td>\n",
       "      <td>0.887173</td>\n",
       "      <td>0.574543</td>\n",
       "      <td>0.682575</td>\n",
       "      <td>0.397101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.894400</td>\n",
       "      <td>0.882576</td>\n",
       "      <td>0.576674</td>\n",
       "      <td>0.684836</td>\n",
       "      <td>0.399623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.889900</td>\n",
       "      <td>0.879686</td>\n",
       "      <td>0.579313</td>\n",
       "      <td>0.684778</td>\n",
       "      <td>0.402363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.889200</td>\n",
       "      <td>0.877337</td>\n",
       "      <td>0.580168</td>\n",
       "      <td>0.686851</td>\n",
       "      <td>0.403392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.885400</td>\n",
       "      <td>0.873111</td>\n",
       "      <td>0.582836</td>\n",
       "      <td>0.690287</td>\n",
       "      <td>0.407944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.883300</td>\n",
       "      <td>0.870362</td>\n",
       "      <td>0.583488</td>\n",
       "      <td>0.692563</td>\n",
       "      <td>0.409945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.882300</td>\n",
       "      <td>0.868533</td>\n",
       "      <td>0.584416</td>\n",
       "      <td>0.693375</td>\n",
       "      <td>0.410307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.879500</td>\n",
       "      <td>0.869106</td>\n",
       "      <td>0.584822</td>\n",
       "      <td>0.691186</td>\n",
       "      <td>0.409974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.866608</td>\n",
       "      <td>0.585749</td>\n",
       "      <td>0.692781</td>\n",
       "      <td>0.411757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.864964</td>\n",
       "      <td>0.586170</td>\n",
       "      <td>0.694375</td>\n",
       "      <td>0.413265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.874700</td>\n",
       "      <td>0.863939</td>\n",
       "      <td>0.587330</td>\n",
       "      <td>0.694375</td>\n",
       "      <td>0.414178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.870600</td>\n",
       "      <td>0.865823</td>\n",
       "      <td>0.586213</td>\n",
       "      <td>0.693244</td>\n",
       "      <td>0.412438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.870100</td>\n",
       "      <td>0.861322</td>\n",
       "      <td>0.588591</td>\n",
       "      <td>0.694346</td>\n",
       "      <td>0.415381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.867800</td>\n",
       "      <td>0.860880</td>\n",
       "      <td>0.588518</td>\n",
       "      <td>0.695288</td>\n",
       "      <td>0.415483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.867400</td>\n",
       "      <td>0.860252</td>\n",
       "      <td>0.588968</td>\n",
       "      <td>0.695854</td>\n",
       "      <td>0.416758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.869500</td>\n",
       "      <td>0.860449</td>\n",
       "      <td>0.588547</td>\n",
       "      <td>0.693665</td>\n",
       "      <td>0.415715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.865600</td>\n",
       "      <td>0.859351</td>\n",
       "      <td>0.589562</td>\n",
       "      <td>0.695607</td>\n",
       "      <td>0.416382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>0.858525</td>\n",
       "      <td>0.589649</td>\n",
       "      <td>0.695825</td>\n",
       "      <td>0.416758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.868200</td>\n",
       "      <td>0.858119</td>\n",
       "      <td>0.589997</td>\n",
       "      <td>0.695288</td>\n",
       "      <td>0.417193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.858468</td>\n",
       "      <td>0.590476</td>\n",
       "      <td>0.695303</td>\n",
       "      <td>0.417266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.867900</td>\n",
       "      <td>0.857448</td>\n",
       "      <td>0.590649</td>\n",
       "      <td>0.696144</td>\n",
       "      <td>0.417904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.864400</td>\n",
       "      <td>0.857842</td>\n",
       "      <td>0.590577</td>\n",
       "      <td>0.695854</td>\n",
       "      <td>0.417715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>0.857604</td>\n",
       "      <td>0.590722</td>\n",
       "      <td>0.696028</td>\n",
       "      <td>0.417918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8623' max='8623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8623/8623 01:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! Final Joint Acc: N/A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–</td></tr><tr><td>eval/age_acc</td><td>â–â–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/gender_acc</td><td>â–â–ƒâ–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/joint_acc</td><td>â–â–ƒâ–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/loss</td><td>â–ˆâ–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>eval/runtime</td><td>â–‡â–â–‚â–„â–ƒâ–„â–…â–â–‡â–…â–ˆâ–ƒâ–†â–…â–ƒâ–…â–â–…â–†â–‚â–„â–…â–ˆâ–‡â–†â–†â–†â–…â–ˆâ–‚</td></tr><tr><td>eval/samples_per_second</td><td>â–‚â–ˆâ–‡â–„â–†â–…â–„â–ˆâ–‚â–„â–â–†â–ƒâ–„â–†â–„â–ˆâ–„â–ƒâ–‡â–…â–„â–â–‚â–ƒâ–ƒâ–ƒâ–„â–â–‡</td></tr><tr><td>eval/steps_per_second</td><td>â–‚â–ˆâ–‡â–…â–†â–…â–„â–ˆâ–‚â–„â–â–†â–ƒâ–„â–†â–„â–ˆâ–„â–ƒâ–‡â–…â–„â–â–‚â–ƒâ–ƒâ–ƒâ–„â–â–‡</td></tr><tr><td>eval_age_acc</td><td>â–</td></tr><tr><td>eval_gender_acc</td><td>â–</td></tr><tr><td>eval_joint_acc</td><td>â–</td></tr><tr><td>eval_loss</td><td>â–</td></tr><tr><td>eval_runtime</td><td>â–</td></tr><tr><td>eval_samples_per_second</td><td>â–</td></tr><tr><td>eval_steps_per_second</td><td>â–</td></tr><tr><td>train/epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–‡â–†â–†â–â–ƒâ–„â–†â–ƒâ–ƒâ–„â–‚â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–„â–ƒâ–ƒâ–â–„â–„â–ƒâ–…â–ƒâ–‡â–„</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>eval/age_acc</td><td>0.59081</td></tr><tr><td>eval/gender_acc</td><td>0.69616</td></tr><tr><td>eval/joint_acc</td><td>0.41808</td></tr><tr><td>eval/loss</td><td>0.85755</td></tr><tr><td>eval/runtime</td><td>81.9993</td></tr><tr><td>eval/samples_per_second</td><td>841.226</td></tr><tr><td>eval/steps_per_second</td><td>105.159</td></tr><tr><td>eval_age_acc</td><td>0.59081</td></tr><tr><td>eval_gender_acc</td><td>0.69616</td></tr><tr><td>eval_joint_acc</td><td>0.41808</td></tr><tr><td>eval_loss</td><td>0.85755</td></tr><tr><td>eval_runtime</td><td>81.9993</td></tr><tr><td>eval_samples_per_second</td><td>841.226</td></tr><tr><td>eval_steps_per_second</td><td>105.159</td></tr><tr><td>total_flos</td><td>4.911087437562839e+17</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>29103</td></tr><tr><td>train/grad_norm</td><td>2.20232</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.8675</td></tr><tr><td>train_loss</td><td>0.8897</td></tr><tr><td>train_runtime</td><td>8849.4246</td></tr><tr><td>train_samples_per_second</td><td>210.458</td></tr><tr><td>train_steps_per_second</td><td>3.289</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">apricot-sweep-1</strong> at: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/oyjz2f8m' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/oyjz2f8m</a><br> View project at: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250828_194432-oyjz2f8m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nlttzuec with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tage_alpha: 0.480985483564206\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.903238103779662e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpeft_r_value: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/work/ULM-25-authorship-profiling/src/wandb/run-20250828_221327-nlttzuec</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/nlttzuec' target=\"_blank\">logical-sweep-2</a></strong> to <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/nlttzuec' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/nlttzuec</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38804' max='38804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38804/38804 3:15:32, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Age Acc</th>\n",
       "      <th>Gender Acc</th>\n",
       "      <th>Joint Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.037900</td>\n",
       "      <td>0.945776</td>\n",
       "      <td>0.524964</td>\n",
       "      <td>0.639388</td>\n",
       "      <td>0.336257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.932400</td>\n",
       "      <td>0.906891</td>\n",
       "      <td>0.552609</td>\n",
       "      <td>0.657335</td>\n",
       "      <td>0.365120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.905600</td>\n",
       "      <td>0.887276</td>\n",
       "      <td>0.561177</td>\n",
       "      <td>0.671296</td>\n",
       "      <td>0.379443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>0.877940</td>\n",
       "      <td>0.566469</td>\n",
       "      <td>0.677095</td>\n",
       "      <td>0.386808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.887000</td>\n",
       "      <td>0.871810</td>\n",
       "      <td>0.568832</td>\n",
       "      <td>0.678863</td>\n",
       "      <td>0.390794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.877000</td>\n",
       "      <td>0.866639</td>\n",
       "      <td>0.571731</td>\n",
       "      <td>0.681357</td>\n",
       "      <td>0.394622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.872200</td>\n",
       "      <td>0.863836</td>\n",
       "      <td>0.574195</td>\n",
       "      <td>0.682256</td>\n",
       "      <td>0.397883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.870400</td>\n",
       "      <td>0.859057</td>\n",
       "      <td>0.576718</td>\n",
       "      <td>0.685590</td>\n",
       "      <td>0.400681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.866600</td>\n",
       "      <td>0.857076</td>\n",
       "      <td>0.577733</td>\n",
       "      <td>0.684633</td>\n",
       "      <td>0.401508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.865900</td>\n",
       "      <td>0.853903</td>\n",
       "      <td>0.580255</td>\n",
       "      <td>0.686779</td>\n",
       "      <td>0.404146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.861300</td>\n",
       "      <td>0.850131</td>\n",
       "      <td>0.580908</td>\n",
       "      <td>0.688895</td>\n",
       "      <td>0.406756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.859900</td>\n",
       "      <td>0.847733</td>\n",
       "      <td>0.582009</td>\n",
       "      <td>0.690751</td>\n",
       "      <td>0.407770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.846226</td>\n",
       "      <td>0.582299</td>\n",
       "      <td>0.691954</td>\n",
       "      <td>0.408365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.856900</td>\n",
       "      <td>0.846892</td>\n",
       "      <td>0.583894</td>\n",
       "      <td>0.688678</td>\n",
       "      <td>0.408829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.851400</td>\n",
       "      <td>0.843898</td>\n",
       "      <td>0.584314</td>\n",
       "      <td>0.692317</td>\n",
       "      <td>0.410655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.847800</td>\n",
       "      <td>0.841992</td>\n",
       "      <td>0.585039</td>\n",
       "      <td>0.693549</td>\n",
       "      <td>0.413381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.851700</td>\n",
       "      <td>0.841289</td>\n",
       "      <td>0.585547</td>\n",
       "      <td>0.693824</td>\n",
       "      <td>0.412873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.847500</td>\n",
       "      <td>0.844162</td>\n",
       "      <td>0.583749</td>\n",
       "      <td>0.692317</td>\n",
       "      <td>0.410771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.846800</td>\n",
       "      <td>0.838328</td>\n",
       "      <td>0.587141</td>\n",
       "      <td>0.694462</td>\n",
       "      <td>0.415367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.844100</td>\n",
       "      <td>0.838221</td>\n",
       "      <td>0.586518</td>\n",
       "      <td>0.695028</td>\n",
       "      <td>0.414540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.836629</td>\n",
       "      <td>0.588475</td>\n",
       "      <td>0.695491</td>\n",
       "      <td>0.416874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.844500</td>\n",
       "      <td>0.837089</td>\n",
       "      <td>0.587982</td>\n",
       "      <td>0.694259</td>\n",
       "      <td>0.416382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.842100</td>\n",
       "      <td>0.834665</td>\n",
       "      <td>0.588939</td>\n",
       "      <td>0.696506</td>\n",
       "      <td>0.417686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.842800</td>\n",
       "      <td>0.834746</td>\n",
       "      <td>0.589214</td>\n",
       "      <td>0.695825</td>\n",
       "      <td>0.417251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.844100</td>\n",
       "      <td>0.834041</td>\n",
       "      <td>0.590070</td>\n",
       "      <td>0.695665</td>\n",
       "      <td>0.418208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.844900</td>\n",
       "      <td>0.835110</td>\n",
       "      <td>0.589316</td>\n",
       "      <td>0.694477</td>\n",
       "      <td>0.416860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>0.832721</td>\n",
       "      <td>0.590533</td>\n",
       "      <td>0.696303</td>\n",
       "      <td>0.418948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.839500</td>\n",
       "      <td>0.832646</td>\n",
       "      <td>0.590823</td>\n",
       "      <td>0.695665</td>\n",
       "      <td>0.418977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.841500</td>\n",
       "      <td>0.831448</td>\n",
       "      <td>0.590591</td>\n",
       "      <td>0.697188</td>\n",
       "      <td>0.419774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.835600</td>\n",
       "      <td>0.832614</td>\n",
       "      <td>0.589287</td>\n",
       "      <td>0.696086</td>\n",
       "      <td>0.417759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.831838</td>\n",
       "      <td>0.590084</td>\n",
       "      <td>0.696608</td>\n",
       "      <td>0.418701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.839700</td>\n",
       "      <td>0.831965</td>\n",
       "      <td>0.590939</td>\n",
       "      <td>0.696448</td>\n",
       "      <td>0.418861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.839700</td>\n",
       "      <td>0.830262</td>\n",
       "      <td>0.590983</td>\n",
       "      <td>0.696666</td>\n",
       "      <td>0.419600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.838200</td>\n",
       "      <td>0.831535</td>\n",
       "      <td>0.591389</td>\n",
       "      <td>0.695129</td>\n",
       "      <td>0.418759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.838600</td>\n",
       "      <td>0.830988</td>\n",
       "      <td>0.591273</td>\n",
       "      <td>0.696608</td>\n",
       "      <td>0.419382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.836900</td>\n",
       "      <td>0.830943</td>\n",
       "      <td>0.591548</td>\n",
       "      <td>0.696434</td>\n",
       "      <td>0.419629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.837400</td>\n",
       "      <td>0.830980</td>\n",
       "      <td>0.591084</td>\n",
       "      <td>0.697130</td>\n",
       "      <td>0.419571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.842500</td>\n",
       "      <td>0.830090</td>\n",
       "      <td>0.591215</td>\n",
       "      <td>0.696825</td>\n",
       "      <td>0.419817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8623' max='8623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8623/8623 01:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! Final Joint Acc: N/A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–</td></tr><tr><td>eval/age_acc</td><td>â–â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/gender_acc</td><td>â–â–ƒâ–…â–†â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/joint_acc</td><td>â–â–ƒâ–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/loss</td><td>â–ˆâ–†â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>eval/runtime</td><td>â–ˆâ–ƒâ–„â–…â–„â–…â–…â–…â–„â–‡â–â–„â–ƒâ–†â–„â–…â–…â–†â–†â–†â–†â–„â–ƒâ–…â–‚â–ƒâ–‡â–†â–…â–„â–†â–†â–†â–ƒâ–‡â–†â–‡â–„â–‚</td></tr><tr><td>eval/samples_per_second</td><td>â–â–†â–…â–„â–„â–„â–„â–„â–…â–â–ˆâ–…â–†â–ƒâ–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–…â–†â–„â–‡â–†â–‚â–ƒâ–„â–…â–ƒâ–ƒâ–ƒâ–†â–‚â–ƒâ–‚â–…â–‡</td></tr><tr><td>eval/steps_per_second</td><td>â–â–†â–…â–„â–…â–„â–„â–„â–…â–â–ˆâ–…â–†â–ƒâ–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–…â–†â–„â–‡â–†â–‚â–ƒâ–„â–…â–ƒâ–ƒâ–ƒâ–†â–‚â–ƒâ–‚â–…â–‡</td></tr><tr><td>eval_age_acc</td><td>â–</td></tr><tr><td>eval_gender_acc</td><td>â–</td></tr><tr><td>eval_joint_acc</td><td>â–</td></tr><tr><td>eval_loss</td><td>â–</td></tr><tr><td>eval_runtime</td><td>â–</td></tr><tr><td>eval_samples_per_second</td><td>â–</td></tr><tr><td>eval_steps_per_second</td><td>â–</td></tr><tr><td>train/epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–ˆâ–…â–†â–‚â–ƒâ–ƒâ–…â–ƒâ–‚â–‚â–‚â–ˆâ–‚â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–„â–ƒâ–„â–ƒâ–…â–„â–…â–‡â–‚â–„â–ƒâ–ƒâ–‚â–„â–…</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>eval/age_acc</td><td>0.59142</td></tr><tr><td>eval/gender_acc</td><td>0.69723</td></tr><tr><td>eval/joint_acc</td><td>0.42014</td></tr><tr><td>eval/loss</td><td>0.83017</td></tr><tr><td>eval/runtime</td><td>81.3075</td></tr><tr><td>eval/samples_per_second</td><td>848.384</td></tr><tr><td>eval/steps_per_second</td><td>106.054</td></tr><tr><td>eval_age_acc</td><td>0.59142</td></tr><tr><td>eval_gender_acc</td><td>0.69723</td></tr><tr><td>eval_joint_acc</td><td>0.42014</td></tr><tr><td>eval_loss</td><td>0.83017</td></tr><tr><td>eval_runtime</td><td>81.3075</td></tr><tr><td>eval_samples_per_second</td><td>848.384</td></tr><tr><td>eval_steps_per_second</td><td>106.054</td></tr><tr><td>total_flos</td><td>6.548116583417119e+17</td></tr><tr><td>train/epoch</td><td>4</td></tr><tr><td>train/global_step</td><td>38804</td></tr><tr><td>train/grad_norm</td><td>2.38462</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.8425</td></tr><tr><td>train_loss</td><td>0.85913</td></tr><tr><td>train_runtime</td><td>11732.7467</td></tr><tr><td>train_samples_per_second</td><td>211.651</td></tr><tr><td>train_steps_per_second</td><td>3.307</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">logical-sweep-2</strong> at: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/nlttzuec' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/nlttzuec</a><br> View project at: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250828_221327-nlttzuec/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 76jcn6eq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tage_alpha: 0.8537993281786413\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.90563000535912e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpeft_r_value: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/work/ULM-25-authorship-profiling/src/wandb/run-20250829_013027-76jcn6eq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/76jcn6eq' target=\"_blank\">usual-sweep-3</a></strong> to <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/76jcn6eq' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/76jcn6eq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19404' max='19404' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19404/19404 3:13:48, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Age Acc</th>\n",
       "      <th>Gender Acc</th>\n",
       "      <th>Joint Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.252900</td>\n",
       "      <td>1.122801</td>\n",
       "      <td>0.554625</td>\n",
       "      <td>0.650768</td>\n",
       "      <td>0.362424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.111800</td>\n",
       "      <td>1.088208</td>\n",
       "      <td>0.569991</td>\n",
       "      <td>0.663627</td>\n",
       "      <td>0.381212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.092400</td>\n",
       "      <td>1.073313</td>\n",
       "      <td>0.575500</td>\n",
       "      <td>0.670281</td>\n",
       "      <td>0.388837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.077800</td>\n",
       "      <td>1.062691</td>\n",
       "      <td>0.579820</td>\n",
       "      <td>0.671398</td>\n",
       "      <td>0.392983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.072400</td>\n",
       "      <td>1.055548</td>\n",
       "      <td>0.583097</td>\n",
       "      <td>0.675689</td>\n",
       "      <td>0.398115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.064700</td>\n",
       "      <td>1.049916</td>\n",
       "      <td>0.584633</td>\n",
       "      <td>0.677312</td>\n",
       "      <td>0.400536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.059700</td>\n",
       "      <td>1.044973</td>\n",
       "      <td>0.587634</td>\n",
       "      <td>0.676210</td>\n",
       "      <td>0.402566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.049100</td>\n",
       "      <td>1.040900</td>\n",
       "      <td>0.589040</td>\n",
       "      <td>0.677878</td>\n",
       "      <td>0.404407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.048000</td>\n",
       "      <td>1.040058</td>\n",
       "      <td>0.589113</td>\n",
       "      <td>0.680009</td>\n",
       "      <td>0.406161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.042700</td>\n",
       "      <td>1.033717</td>\n",
       "      <td>0.592012</td>\n",
       "      <td>0.680632</td>\n",
       "      <td>0.407553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.043200</td>\n",
       "      <td>1.032581</td>\n",
       "      <td>0.591548</td>\n",
       "      <td>0.679545</td>\n",
       "      <td>0.408220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.039600</td>\n",
       "      <td>1.029465</td>\n",
       "      <td>0.594042</td>\n",
       "      <td>0.681415</td>\n",
       "      <td>0.409756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.040400</td>\n",
       "      <td>1.028900</td>\n",
       "      <td>0.594216</td>\n",
       "      <td>0.682676</td>\n",
       "      <td>0.411177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.037400</td>\n",
       "      <td>1.026799</td>\n",
       "      <td>0.594984</td>\n",
       "      <td>0.682241</td>\n",
       "      <td>0.411554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.034700</td>\n",
       "      <td>1.026983</td>\n",
       "      <td>0.594984</td>\n",
       "      <td>0.682444</td>\n",
       "      <td>0.411569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.034400</td>\n",
       "      <td>1.024633</td>\n",
       "      <td>0.595883</td>\n",
       "      <td>0.682763</td>\n",
       "      <td>0.412757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.034800</td>\n",
       "      <td>1.023785</td>\n",
       "      <td>0.595926</td>\n",
       "      <td>0.683575</td>\n",
       "      <td>0.412931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.032800</td>\n",
       "      <td>1.024207</td>\n",
       "      <td>0.596144</td>\n",
       "      <td>0.682140</td>\n",
       "      <td>0.412670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.036800</td>\n",
       "      <td>1.023037</td>\n",
       "      <td>0.596651</td>\n",
       "      <td>0.683662</td>\n",
       "      <td>0.413845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8623' max='8623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8623/8623 01:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! Final Joint Acc: N/A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–</td></tr><tr><td>eval/age_acc</td><td>â–â–„â–„â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/gender_acc</td><td>â–â–„â–…â–…â–†â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/joint_acc</td><td>â–â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/loss</td><td>â–ˆâ–†â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–</td></tr><tr><td>eval/runtime</td><td>â–„â–„â–â–…â–‚â–ƒâ–„â–ˆâ–…â–â–‡â–‚â–†â–‚â–ˆâ–…â–‡â–ˆâ–‡â–†</td></tr><tr><td>eval/samples_per_second</td><td>â–…â–…â–ˆâ–„â–‡â–†â–…â–â–„â–ˆâ–‚â–‡â–ƒâ–‡â–â–„â–‚â–â–‚â–ƒ</td></tr><tr><td>eval/steps_per_second</td><td>â–…â–…â–ˆâ–„â–‡â–†â–…â–â–„â–ˆâ–‚â–‡â–ƒâ–‡â–â–„â–‚â–â–‚â–ƒ</td></tr><tr><td>eval_age_acc</td><td>â–</td></tr><tr><td>eval_gender_acc</td><td>â–</td></tr><tr><td>eval_joint_acc</td><td>â–</td></tr><tr><td>eval_loss</td><td>â–</td></tr><tr><td>eval_runtime</td><td>â–</td></tr><tr><td>eval_samples_per_second</td><td>â–</td></tr><tr><td>eval_steps_per_second</td><td>â–</td></tr><tr><td>train/epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–‚â–‚â–‚â–â–„â–‚â–ƒâ–„â–â–ˆâ–ƒâ–‚â–ƒâ–†â–‚â–ƒâ–‚â–‚â–</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>eval/age_acc</td><td>0.59625</td></tr><tr><td>eval/gender_acc</td><td>0.68304</td></tr><tr><td>eval/joint_acc</td><td>0.41326</td></tr><tr><td>eval/loss</td><td>1.0231</td></tr><tr><td>eval/runtime</td><td>82.8978</td></tr><tr><td>eval/samples_per_second</td><td>832.109</td></tr><tr><td>eval/steps_per_second</td><td>104.02</td></tr><tr><td>eval_age_acc</td><td>0.59625</td></tr><tr><td>eval_gender_acc</td><td>0.68304</td></tr><tr><td>eval_joint_acc</td><td>0.41326</td></tr><tr><td>eval_loss</td><td>1.0231</td></tr><tr><td>eval_runtime</td><td>82.8978</td></tr><tr><td>eval_samples_per_second</td><td>832.109</td></tr><tr><td>eval_steps_per_second</td><td>104.02</td></tr><tr><td>total_flos</td><td>6.548116583417119e+17</td></tr><tr><td>train/epoch</td><td>4</td></tr><tr><td>train/global_step</td><td>19404</td></tr><tr><td>train/grad_norm</td><td>1.68923</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.0368</td></tr><tr><td>train_loss</td><td>1.06287</td></tr><tr><td>train_runtime</td><td>11629.1685</td></tr><tr><td>train_samples_per_second</td><td>213.536</td></tr><tr><td>train_steps_per_second</td><td>1.669</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">usual-sweep-3</strong> at: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/76jcn6eq' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/76jcn6eq</a><br> View project at: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250829_013027-76jcn6eq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3vltbdyd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tage_alpha: 0.4120563317096145\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8.586771270684372e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpeft_r_value: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/work/ULM-25-authorship-profiling/src/wandb/run-20250829_044552-3vltbdyd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/3vltbdyd' target=\"_blank\">divine-sweep-4</a></strong> to <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/3vltbdyd' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/3vltbdyd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19402' max='19402' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19402/19402 1:37:49, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Age Acc</th>\n",
       "      <th>Gender Acc</th>\n",
       "      <th>Joint Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.965300</td>\n",
       "      <td>0.877340</td>\n",
       "      <td>0.543520</td>\n",
       "      <td>0.655175</td>\n",
       "      <td>0.355842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.868100</td>\n",
       "      <td>0.848155</td>\n",
       "      <td>0.560235</td>\n",
       "      <td>0.671470</td>\n",
       "      <td>0.378675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.851100</td>\n",
       "      <td>0.835470</td>\n",
       "      <td>0.567048</td>\n",
       "      <td>0.681255</td>\n",
       "      <td>0.390591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.839900</td>\n",
       "      <td>0.828326</td>\n",
       "      <td>0.569948</td>\n",
       "      <td>0.685170</td>\n",
       "      <td>0.395173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.837200</td>\n",
       "      <td>0.824362</td>\n",
       "      <td>0.572166</td>\n",
       "      <td>0.684764</td>\n",
       "      <td>0.397695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.829200</td>\n",
       "      <td>0.819459</td>\n",
       "      <td>0.574601</td>\n",
       "      <td>0.688315</td>\n",
       "      <td>0.400841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.825500</td>\n",
       "      <td>0.817959</td>\n",
       "      <td>0.576428</td>\n",
       "      <td>0.687591</td>\n",
       "      <td>0.402247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.824600</td>\n",
       "      <td>0.814432</td>\n",
       "      <td>0.579356</td>\n",
       "      <td>0.689736</td>\n",
       "      <td>0.405973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.821300</td>\n",
       "      <td>0.813171</td>\n",
       "      <td>0.580226</td>\n",
       "      <td>0.688939</td>\n",
       "      <td>0.406335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.820600</td>\n",
       "      <td>0.809783</td>\n",
       "      <td>0.582154</td>\n",
       "      <td>0.692360</td>\n",
       "      <td>0.408742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>0.807990</td>\n",
       "      <td>0.582299</td>\n",
       "      <td>0.692592</td>\n",
       "      <td>0.410090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.816900</td>\n",
       "      <td>0.805672</td>\n",
       "      <td>0.582459</td>\n",
       "      <td>0.694578</td>\n",
       "      <td>0.411148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.816600</td>\n",
       "      <td>0.804058</td>\n",
       "      <td>0.583198</td>\n",
       "      <td>0.695680</td>\n",
       "      <td>0.411801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.806641</td>\n",
       "      <td>0.583575</td>\n",
       "      <td>0.692215</td>\n",
       "      <td>0.410481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.811100</td>\n",
       "      <td>0.804271</td>\n",
       "      <td>0.584111</td>\n",
       "      <td>0.694419</td>\n",
       "      <td>0.411786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.807800</td>\n",
       "      <td>0.803101</td>\n",
       "      <td>0.584575</td>\n",
       "      <td>0.694955</td>\n",
       "      <td>0.413250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.812100</td>\n",
       "      <td>0.802145</td>\n",
       "      <td>0.584836</td>\n",
       "      <td>0.695897</td>\n",
       "      <td>0.414280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.802869</td>\n",
       "      <td>0.585836</td>\n",
       "      <td>0.695390</td>\n",
       "      <td>0.413903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.809500</td>\n",
       "      <td>0.802509</td>\n",
       "      <td>0.585329</td>\n",
       "      <td>0.695216</td>\n",
       "      <td>0.413859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8623' max='8623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8623/8623 01:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! Final Joint Acc: N/A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–</td></tr><tr><td>eval/age_acc</td><td>â–â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/gender_acc</td><td>â–â–„â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/joint_acc</td><td>â–â–„â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/loss</td><td>â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–</td></tr><tr><td>eval/runtime</td><td>â–ƒâ–„â–„â–„â–„â–†â–‚â–„â–ƒâ–„â–…â–ƒâ–„â–ƒâ–…â–â–‚â–‚â–„â–ˆ</td></tr><tr><td>eval/samples_per_second</td><td>â–†â–…â–…â–…â–…â–ƒâ–‡â–…â–†â–„â–„â–†â–…â–†â–„â–ˆâ–‡â–‡â–…â–</td></tr><tr><td>eval/steps_per_second</td><td>â–†â–…â–…â–…â–…â–ƒâ–‡â–…â–†â–„â–„â–†â–…â–†â–„â–ˆâ–‡â–‡â–…â–</td></tr><tr><td>eval_age_acc</td><td>â–</td></tr><tr><td>eval_gender_acc</td><td>â–</td></tr><tr><td>eval_joint_acc</td><td>â–</td></tr><tr><td>eval_loss</td><td>â–</td></tr><tr><td>eval_runtime</td><td>â–</td></tr><tr><td>eval_samples_per_second</td><td>â–</td></tr><tr><td>eval_steps_per_second</td><td>â–</td></tr><tr><td>train/epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–ˆâ–…â–„â–â–‚â–‚â–„â–â–â–â–â–…â–â–‚â–‚â–‚â–‚â–ƒâ–‚</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>eval/age_acc</td><td>0.58543</td></tr><tr><td>eval/gender_acc</td><td>0.69571</td></tr><tr><td>eval/joint_acc</td><td>0.41445</td></tr><tr><td>eval/loss</td><td>0.80203</td></tr><tr><td>eval/runtime</td><td>82.4807</td></tr><tr><td>eval/samples_per_second</td><td>836.317</td></tr><tr><td>eval/steps_per_second</td><td>104.546</td></tr><tr><td>eval_age_acc</td><td>0.58543</td></tr><tr><td>eval_gender_acc</td><td>0.69571</td></tr><tr><td>eval_joint_acc</td><td>0.41445</td></tr><tr><td>eval_loss</td><td>0.80203</td></tr><tr><td>eval_runtime</td><td>82.4807</td></tr><tr><td>eval_samples_per_second</td><td>836.317</td></tr><tr><td>eval_steps_per_second</td><td>104.546</td></tr><tr><td>total_flos</td><td>3.2740582917085594e+17</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/global_step</td><td>19402</td></tr><tr><td>train/grad_norm</td><td>1.99939</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.8095</td></tr><tr><td>train_loss</td><td>0.83104</td></tr><tr><td>train_runtime</td><td>5869.9665</td></tr><tr><td>train_samples_per_second</td><td>211.521</td></tr><tr><td>train_steps_per_second</td><td>3.305</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-sweep-4</strong> at: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/3vltbdyd' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/3vltbdyd</a><br> View project at: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250829_044552-3vltbdyd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2724witm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tage_alpha: 0.7513140961141846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 7.030190749062255e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpeft_r_value: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/work/ULM-25-authorship-profiling/src/wandb/run-20250829_062509-2724witm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/2724witm' target=\"_blank\">vague-sweep-5</a></strong> to <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/2724witm' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/2724witm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45515' max='58203' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45515/58203 2:21:54 < 39:33, 5.35 it/s, Epoch 2.35/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Age Acc</th>\n",
       "      <th>Gender Acc</th>\n",
       "      <th>Joint Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.243900</td>\n",
       "      <td>1.111436</td>\n",
       "      <td>0.527138</td>\n",
       "      <td>0.646158</td>\n",
       "      <td>0.339664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.089500</td>\n",
       "      <td>1.067117</td>\n",
       "      <td>0.552334</td>\n",
       "      <td>0.654320</td>\n",
       "      <td>0.362801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.059600</td>\n",
       "      <td>1.046658</td>\n",
       "      <td>0.560148</td>\n",
       "      <td>0.666454</td>\n",
       "      <td>0.375993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.056100</td>\n",
       "      <td>1.036267</td>\n",
       "      <td>0.565845</td>\n",
       "      <td>0.670586</td>\n",
       "      <td>0.383836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.044800</td>\n",
       "      <td>1.028552</td>\n",
       "      <td>0.569136</td>\n",
       "      <td>0.672601</td>\n",
       "      <td>0.388011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.034500</td>\n",
       "      <td>1.022261</td>\n",
       "      <td>0.572572</td>\n",
       "      <td>0.673456</td>\n",
       "      <td>0.390476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.026900</td>\n",
       "      <td>1.020252</td>\n",
       "      <td>0.575152</td>\n",
       "      <td>0.668991</td>\n",
       "      <td>0.389475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.022200</td>\n",
       "      <td>1.014190</td>\n",
       "      <td>0.576109</td>\n",
       "      <td>0.677979</td>\n",
       "      <td>0.396231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.033400</td>\n",
       "      <td>1.009665</td>\n",
       "      <td>0.579211</td>\n",
       "      <td>0.677486</td>\n",
       "      <td>0.398304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.017200</td>\n",
       "      <td>1.008479</td>\n",
       "      <td>0.579197</td>\n",
       "      <td>0.678008</td>\n",
       "      <td>0.399377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.013200</td>\n",
       "      <td>1.004323</td>\n",
       "      <td>0.581139</td>\n",
       "      <td>0.680241</td>\n",
       "      <td>0.402189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.007600</td>\n",
       "      <td>1.001326</td>\n",
       "      <td>0.581502</td>\n",
       "      <td>0.683589</td>\n",
       "      <td>0.403175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.007200</td>\n",
       "      <td>0.998442</td>\n",
       "      <td>0.582836</td>\n",
       "      <td>0.682430</td>\n",
       "      <td>0.405436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.009700</td>\n",
       "      <td>0.996777</td>\n",
       "      <td>0.584329</td>\n",
       "      <td>0.681937</td>\n",
       "      <td>0.405929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.004100</td>\n",
       "      <td>0.993978</td>\n",
       "      <td>0.585170</td>\n",
       "      <td>0.684793</td>\n",
       "      <td>0.408437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.009800</td>\n",
       "      <td>0.992486</td>\n",
       "      <td>0.585184</td>\n",
       "      <td>0.684155</td>\n",
       "      <td>0.407002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.007600</td>\n",
       "      <td>0.993432</td>\n",
       "      <td>0.584358</td>\n",
       "      <td>0.684358</td>\n",
       "      <td>0.406959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>0.989616</td>\n",
       "      <td>0.586895</td>\n",
       "      <td>0.682966</td>\n",
       "      <td>0.408292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.001700</td>\n",
       "      <td>0.991521</td>\n",
       "      <td>0.585967</td>\n",
       "      <td>0.686170</td>\n",
       "      <td>0.408843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.001200</td>\n",
       "      <td>0.987864</td>\n",
       "      <td>0.586286</td>\n",
       "      <td>0.684590</td>\n",
       "      <td>0.408539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.001100</td>\n",
       "      <td>0.984241</td>\n",
       "      <td>0.588750</td>\n",
       "      <td>0.687576</td>\n",
       "      <td>0.412627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.994500</td>\n",
       "      <td>0.983565</td>\n",
       "      <td>0.590476</td>\n",
       "      <td>0.686634</td>\n",
       "      <td>0.413337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.993200</td>\n",
       "      <td>0.981996</td>\n",
       "      <td>0.590186</td>\n",
       "      <td>0.688373</td>\n",
       "      <td>0.413337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>0.980939</td>\n",
       "      <td>0.590504</td>\n",
       "      <td>0.688750</td>\n",
       "      <td>0.414164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.995500</td>\n",
       "      <td>0.979526</td>\n",
       "      <td>0.590838</td>\n",
       "      <td>0.690128</td>\n",
       "      <td>0.413743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.990500</td>\n",
       "      <td>0.978532</td>\n",
       "      <td>0.590707</td>\n",
       "      <td>0.690171</td>\n",
       "      <td>0.414424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.988500</td>\n",
       "      <td>0.979424</td>\n",
       "      <td>0.591519</td>\n",
       "      <td>0.688634</td>\n",
       "      <td>0.414091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.978644</td>\n",
       "      <td>0.590838</td>\n",
       "      <td>0.687025</td>\n",
       "      <td>0.412815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.989600</td>\n",
       "      <td>0.978189</td>\n",
       "      <td>0.592259</td>\n",
       "      <td>0.686692</td>\n",
       "      <td>0.414207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.981400</td>\n",
       "      <td>0.976606</td>\n",
       "      <td>0.592288</td>\n",
       "      <td>0.687692</td>\n",
       "      <td>0.414381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.981800</td>\n",
       "      <td>0.975002</td>\n",
       "      <td>0.592954</td>\n",
       "      <td>0.688373</td>\n",
       "      <td>0.415526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.980600</td>\n",
       "      <td>0.974529</td>\n",
       "      <td>0.594114</td>\n",
       "      <td>0.689345</td>\n",
       "      <td>0.417367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.988300</td>\n",
       "      <td>0.975455</td>\n",
       "      <td>0.594245</td>\n",
       "      <td>0.685112</td>\n",
       "      <td>0.414424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.982100</td>\n",
       "      <td>0.974915</td>\n",
       "      <td>0.593056</td>\n",
       "      <td>0.689272</td>\n",
       "      <td>0.415816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.981200</td>\n",
       "      <td>0.972074</td>\n",
       "      <td>0.595288</td>\n",
       "      <td>0.690925</td>\n",
       "      <td>0.418817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.980200</td>\n",
       "      <td>0.977663</td>\n",
       "      <td>0.590968</td>\n",
       "      <td>0.687054</td>\n",
       "      <td>0.413395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.979900</td>\n",
       "      <td>0.974150</td>\n",
       "      <td>0.594564</td>\n",
       "      <td>0.690591</td>\n",
       "      <td>0.418063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.979600</td>\n",
       "      <td>0.970809</td>\n",
       "      <td>0.594274</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.417904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.975200</td>\n",
       "      <td>0.970442</td>\n",
       "      <td>0.594709</td>\n",
       "      <td>0.690186</td>\n",
       "      <td>0.417512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.979200</td>\n",
       "      <td>0.971449</td>\n",
       "      <td>0.593969</td>\n",
       "      <td>0.691838</td>\n",
       "      <td>0.417628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.982300</td>\n",
       "      <td>0.969008</td>\n",
       "      <td>0.595926</td>\n",
       "      <td>0.687054</td>\n",
       "      <td>0.417585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.976500</td>\n",
       "      <td>0.968890</td>\n",
       "      <td>0.596506</td>\n",
       "      <td>0.691432</td>\n",
       "      <td>0.420049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.978200</td>\n",
       "      <td>0.969360</td>\n",
       "      <td>0.595506</td>\n",
       "      <td>0.692056</td>\n",
       "      <td>0.418962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.978400</td>\n",
       "      <td>0.969784</td>\n",
       "      <td>0.594854</td>\n",
       "      <td>0.688069</td>\n",
       "      <td>0.416918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.974800</td>\n",
       "      <td>0.968325</td>\n",
       "      <td>0.596651</td>\n",
       "      <td>0.690780</td>\n",
       "      <td>0.419745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8623' max='8623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8623/8623 01:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! Final Joint Acc: N/A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–</td></tr><tr><td>eval/age_acc</td><td>â–â–ƒâ–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/gender_acc</td><td>â–â–„â–…â–†â–†â–‡â–‡â–†â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/joint_acc</td><td>â–â–„â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/loss</td><td>â–ˆâ–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–</td></tr><tr><td>eval/runtime</td><td>â–â–ˆâ–ƒâ–„â–ƒâ–…â–„â–‡â–„â–…â–„â–†â–‡â–…â–ˆâ–„â–„â–‡â–†â–†</td></tr><tr><td>eval/samples_per_second</td><td>â–ˆâ–â–†â–…â–…â–„â–…â–‚â–…â–„â–…â–ƒâ–‚â–„â–â–…â–…â–‚â–ƒâ–ƒ</td></tr><tr><td>eval/steps_per_second</td><td>â–ˆâ–â–†â–…â–…â–„â–…â–‚â–…â–„â–…â–ƒâ–‚â–„â–â–…â–…â–‚â–ƒâ–ƒ</td></tr><tr><td>eval_age_acc</td><td>â–</td></tr><tr><td>eval_gender_acc</td><td>â–</td></tr><tr><td>eval_joint_acc</td><td>â–</td></tr><tr><td>eval_loss</td><td>â–</td></tr><tr><td>eval_runtime</td><td>â–</td></tr><tr><td>eval_samples_per_second</td><td>â–</td></tr><tr><td>eval_steps_per_second</td><td>â–</td></tr><tr><td>train/epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–…â–…â–‡â–â–ƒâ–„â–…â–‚â–‚â–‚â–â–ˆâ–‚â–„â–„â–ƒâ–„â–‚â–‚</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>eval/age_acc</td><td>0.58974</td></tr><tr><td>eval/gender_acc</td><td>0.69198</td></tr><tr><td>eval/joint_acc</td><td>0.41473</td></tr><tr><td>eval/loss</td><td>0.9125</td></tr><tr><td>eval/runtime</td><td>82.0834</td></tr><tr><td>eval/samples_per_second</td><td>840.365</td></tr><tr><td>eval/steps_per_second</td><td>105.052</td></tr><tr><td>eval_age_acc</td><td>0.58974</td></tr><tr><td>eval_gender_acc</td><td>0.69198</td></tr><tr><td>eval_joint_acc</td><td>0.41473</td></tr><tr><td>eval_loss</td><td>0.9125</td></tr><tr><td>eval_runtime</td><td>82.0834</td></tr><tr><td>eval_samples_per_second</td><td>840.365</td></tr><tr><td>eval_steps_per_second</td><td>105.052</td></tr><tr><td>total_flos</td><td>3.2740582917085594e+17</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/global_step</td><td>19402</td></tr><tr><td>train/grad_norm</td><td>2.10476</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.9201</td></tr><tr><td>train_loss</td><td>0.94518</td></tr><tr><td>train_runtime</td><td>5909.003</td></tr><tr><td>train_samples_per_second</td><td>210.124</td></tr><tr><td>train_steps_per_second</td><td>3.283</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-sweep-6</strong> at: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/vh7kl5xg' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/vh7kl5xg</a><br> View project at: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250829_092841-vh7kl5xg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xk922e4s with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tage_alpha: 0.512584843797316\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5.058408191938331e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpeft_r_value: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/work/ULM-25-authorship-profiling/src/wandb/run-20250829_110846-xk922e4s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/xk922e4s' target=\"_blank\">curious-sweep-7</a></strong> to <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/xk922e4s' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/xk922e4s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29103' max='29103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29103/29103 2:27:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Age Acc</th>\n",
       "      <th>Gender Acc</th>\n",
       "      <th>Joint Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.059500</td>\n",
       "      <td>0.945224</td>\n",
       "      <td>0.535822</td>\n",
       "      <td>0.657524</td>\n",
       "      <td>0.352740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.935500</td>\n",
       "      <td>0.911705</td>\n",
       "      <td>0.556814</td>\n",
       "      <td>0.669498</td>\n",
       "      <td>0.376080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.915100</td>\n",
       "      <td>0.897790</td>\n",
       "      <td>0.564424</td>\n",
       "      <td>0.675979</td>\n",
       "      <td>0.386373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.901800</td>\n",
       "      <td>0.891227</td>\n",
       "      <td>0.566932</td>\n",
       "      <td>0.679864</td>\n",
       "      <td>0.390548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.900600</td>\n",
       "      <td>0.886016</td>\n",
       "      <td>0.570774</td>\n",
       "      <td>0.681067</td>\n",
       "      <td>0.394303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>0.881750</td>\n",
       "      <td>0.573239</td>\n",
       "      <td>0.683531</td>\n",
       "      <td>0.397333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.879847</td>\n",
       "      <td>0.574688</td>\n",
       "      <td>0.683010</td>\n",
       "      <td>0.399507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.887000</td>\n",
       "      <td>0.875918</td>\n",
       "      <td>0.577298</td>\n",
       "      <td>0.685720</td>\n",
       "      <td>0.402870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.884300</td>\n",
       "      <td>0.874265</td>\n",
       "      <td>0.578545</td>\n",
       "      <td>0.685648</td>\n",
       "      <td>0.404117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.882900</td>\n",
       "      <td>0.871132</td>\n",
       "      <td>0.580081</td>\n",
       "      <td>0.687794</td>\n",
       "      <td>0.405828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.879800</td>\n",
       "      <td>0.869447</td>\n",
       "      <td>0.581270</td>\n",
       "      <td>0.686648</td>\n",
       "      <td>0.406712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.879300</td>\n",
       "      <td>0.866984</td>\n",
       "      <td>0.581516</td>\n",
       "      <td>0.689504</td>\n",
       "      <td>0.408162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.877800</td>\n",
       "      <td>0.864784</td>\n",
       "      <td>0.583169</td>\n",
       "      <td>0.690099</td>\n",
       "      <td>0.409322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.876400</td>\n",
       "      <td>0.866465</td>\n",
       "      <td>0.583169</td>\n",
       "      <td>0.687707</td>\n",
       "      <td>0.408408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.871700</td>\n",
       "      <td>0.863906</td>\n",
       "      <td>0.583415</td>\n",
       "      <td>0.691055</td>\n",
       "      <td>0.410017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.867600</td>\n",
       "      <td>0.862829</td>\n",
       "      <td>0.584111</td>\n",
       "      <td>0.691548</td>\n",
       "      <td>0.411699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.872700</td>\n",
       "      <td>0.861390</td>\n",
       "      <td>0.585199</td>\n",
       "      <td>0.692056</td>\n",
       "      <td>0.412322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.862637</td>\n",
       "      <td>0.584358</td>\n",
       "      <td>0.691070</td>\n",
       "      <td>0.410699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>0.860101</td>\n",
       "      <td>0.586373</td>\n",
       "      <td>0.692114</td>\n",
       "      <td>0.413685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.866600</td>\n",
       "      <td>0.859490</td>\n",
       "      <td>0.585764</td>\n",
       "      <td>0.692128</td>\n",
       "      <td>0.412830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.866400</td>\n",
       "      <td>0.858961</td>\n",
       "      <td>0.586547</td>\n",
       "      <td>0.693650</td>\n",
       "      <td>0.414526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.867600</td>\n",
       "      <td>0.859406</td>\n",
       "      <td>0.586532</td>\n",
       "      <td>0.692085</td>\n",
       "      <td>0.413700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.865100</td>\n",
       "      <td>0.858684</td>\n",
       "      <td>0.586257</td>\n",
       "      <td>0.692940</td>\n",
       "      <td>0.414019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.866400</td>\n",
       "      <td>0.857813</td>\n",
       "      <td>0.586909</td>\n",
       "      <td>0.693215</td>\n",
       "      <td>0.414424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.867700</td>\n",
       "      <td>0.857613</td>\n",
       "      <td>0.587025</td>\n",
       "      <td>0.692998</td>\n",
       "      <td>0.414482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.868200</td>\n",
       "      <td>0.857843</td>\n",
       "      <td>0.586489</td>\n",
       "      <td>0.692534</td>\n",
       "      <td>0.414019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.867200</td>\n",
       "      <td>0.856966</td>\n",
       "      <td>0.587040</td>\n",
       "      <td>0.693027</td>\n",
       "      <td>0.414758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.863100</td>\n",
       "      <td>0.857320</td>\n",
       "      <td>0.587112</td>\n",
       "      <td>0.693027</td>\n",
       "      <td>0.414584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.867000</td>\n",
       "      <td>0.857206</td>\n",
       "      <td>0.586996</td>\n",
       "      <td>0.692839</td>\n",
       "      <td>0.414511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8623' max='8623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8623/8623 01:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! Final Joint Acc: N/A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–</td></tr><tr><td>eval/age_acc</td><td>â–â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/gender_acc</td><td>â–â–ƒâ–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/joint_acc</td><td>â–â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/loss</td><td>â–ˆâ–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>eval/runtime</td><td>â–ƒâ–ƒâ–…â–…â–„â–‚â–ƒâ–ƒâ–ƒâ–…â–ˆâ–‚â–ƒâ–†â–†â–…â–ƒâ–‡â–†â–‡â–â–‡â–…â–„â–†â–„â–â–†â–„â–…</td></tr><tr><td>eval/samples_per_second</td><td>â–†â–†â–„â–„â–…â–‡â–†â–†â–†â–„â–â–‡â–†â–ƒâ–ƒâ–„â–†â–‚â–ƒâ–‚â–ˆâ–‚â–„â–…â–ƒâ–…â–ˆâ–ƒâ–…â–„</td></tr><tr><td>eval/steps_per_second</td><td>â–†â–†â–„â–„â–…â–‡â–†â–†â–†â–„â–â–‡â–†â–ƒâ–ƒâ–„â–†â–‚â–ƒâ–‚â–ˆâ–‚â–„â–…â–ƒâ–…â–ˆâ–ƒâ–…â–„</td></tr><tr><td>eval_age_acc</td><td>â–</td></tr><tr><td>eval_gender_acc</td><td>â–</td></tr><tr><td>eval_joint_acc</td><td>â–</td></tr><tr><td>eval_loss</td><td>â–</td></tr><tr><td>eval_runtime</td><td>â–</td></tr><tr><td>eval_samples_per_second</td><td>â–</td></tr><tr><td>eval_steps_per_second</td><td>â–</td></tr><tr><td>train/epoch</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–‡â–†â–‡â–‚â–ƒâ–…â–†â–ƒâ–ƒâ–ƒâ–‚â–ˆâ–ƒâ–„â–„â–„â–ƒâ–„â–ƒâ–„â–ƒâ–â–ƒâ–„â–ƒâ–„â–ƒâ–†â–„</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>eval/age_acc</td><td>0.587</td></tr><tr><td>eval/gender_acc</td><td>0.6928</td></tr><tr><td>eval/joint_acc</td><td>0.41448</td></tr><tr><td>eval/loss</td><td>0.85718</td></tr><tr><td>eval/runtime</td><td>82.0824</td></tr><tr><td>eval/samples_per_second</td><td>840.375</td></tr><tr><td>eval/steps_per_second</td><td>105.053</td></tr><tr><td>eval_age_acc</td><td>0.587</td></tr><tr><td>eval_gender_acc</td><td>0.6928</td></tr><tr><td>eval_joint_acc</td><td>0.41448</td></tr><tr><td>eval_loss</td><td>0.85718</td></tr><tr><td>eval_runtime</td><td>82.0824</td></tr><tr><td>eval_samples_per_second</td><td>840.375</td></tr><tr><td>eval_steps_per_second</td><td>105.053</td></tr><tr><td>total_flos</td><td>4.911087437562839e+17</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>29103</td></tr><tr><td>train/grad_norm</td><td>2.17368</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.867</td></tr><tr><td>train_loss</td><td>0.88518</td></tr><tr><td>train_runtime</td><td>8863.2144</td></tr><tr><td>train_samples_per_second</td><td>210.131</td></tr><tr><td>train_steps_per_second</td><td>3.284</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">curious-sweep-7</strong> at: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/xk922e4s' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/xk922e4s</a><br> View project at: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250829_110846-xk922e4s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w1asbw4q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tage_alpha: 0.8378916491056481\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 7.168443274187567e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpeft_r_value: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/work/ULM-25-authorship-profiling/src/wandb/run-20250829_133757-w1asbw4q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/w1asbw4q' target=\"_blank\">rare-sweep-8</a></strong> to <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/sweeps/egrnddjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/w1asbw4q' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/ULM-Author-Profiling/runs/w1asbw4q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8523' max='19404' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8523/19404 1:24:36 < 1:48:02, 1.68 it/s, Epoch 1.76/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Age Acc</th>\n",
       "      <th>Gender Acc</th>\n",
       "      <th>Joint Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.235500</td>\n",
       "      <td>1.100164</td>\n",
       "      <td>0.562873</td>\n",
       "      <td>0.658626</td>\n",
       "      <td>0.373485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.095600</td>\n",
       "      <td>1.074118</td>\n",
       "      <td>0.573485</td>\n",
       "      <td>0.672427</td>\n",
       "      <td>0.389823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>1.062751</td>\n",
       "      <td>0.577515</td>\n",
       "      <td>0.676051</td>\n",
       "      <td>0.395506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.069100</td>\n",
       "      <td>1.054345</td>\n",
       "      <td>0.580415</td>\n",
       "      <td>0.676790</td>\n",
       "      <td>0.398869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.065700</td>\n",
       "      <td>1.049634</td>\n",
       "      <td>0.582227</td>\n",
       "      <td>0.679690</td>\n",
       "      <td>0.400986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.058700</td>\n",
       "      <td>1.043390</td>\n",
       "      <td>0.583981</td>\n",
       "      <td>0.681313</td>\n",
       "      <td>0.404030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.054200</td>\n",
       "      <td>1.040646</td>\n",
       "      <td>0.586155</td>\n",
       "      <td>0.678501</td>\n",
       "      <td>0.403943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.044800</td>\n",
       "      <td>1.038728</td>\n",
       "      <td>0.586228</td>\n",
       "      <td>0.679139</td>\n",
       "      <td>0.404364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train, count=10)  # Will run 10 different hyperparameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b19f45-e639-48ae-a445-ec8afe17cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef35ace4-fc07-4ac8-8779-f93e1928a2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
